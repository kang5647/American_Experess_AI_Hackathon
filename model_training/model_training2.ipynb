{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5e13cf-c8fc-43e8-a025-c3d15a4585c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48c73aa9-17a8-4317-8c85-c57ddb5f873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('train_test/imputed_train.parquet')\n",
    "\n",
    "target_column = 'activation'\n",
    "remove_list = ['activation', 'ind_recommended']\n",
    "features = [col for col in df.columns.to_list() if col not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79ca876c-61a5-4224-bc56-41de2deff972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_init_params(params, verbose=1, cat_idxs=None, cat_dims=None):\n",
    "#     return {\n",
    "#         \"n_d\": params.get(\"n_d\",8),\n",
    "#         \"n_a\": params.get(\"n_d\", 8),\n",
    "#         \"n_steps\": params.get(\"n_steps\", 3),\n",
    "#         \"n_shared\": params.get(\"n_shared\", 2),\n",
    "#         \"cat_emb_dim\": params.get(\"cat_emb_dim\", 1),\n",
    "#         \"optimizer_params\": {\"lr\": params.get(\"lr\", 2e-2)},\n",
    "#         \"mask_type\": params.get(\"mask_type\", \"sparsemax\"),\n",
    "#         \"lambda_sparse\": params.get(\"lambda_sparse\", 1e-3),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"cat_idxs\": cat_idxs or [],\n",
    "#         \"cat_dims\": cat_dims or [],\n",
    "#         \"verbose\": verbose,\n",
    "#     }\n",
    "\n",
    "def get_init_params(verbose=1, cat_idxs=None, cat_dims=None):\n",
    "    return {\n",
    "        \"n_d\": 8,\n",
    "        \"n_a\": 8,\n",
    "        \"n_steps\": 3,\n",
    "        \"n_shared\": 2,\n",
    "        \"cat_emb_dim\": 1,\n",
    "        \"optimizer_params\": {\"lr\": 2e-2},\n",
    "        \"mask_type\": \"sparsemax\",\n",
    "        \"lambda_sparse\": 1e-3,\n",
    "        \"optimizer_fn\": torch.optim.Adam,\n",
    "        \"cat_idxs\": cat_idxs or [],\n",
    "        \"cat_dims\": cat_dims or [],\n",
    "        \"verbose\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c179b7-c276-4807-825b-706acdc7409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464ec82e-a070-44c7-a860-43224a1e571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('train_test/imputed_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131d00f-60f0-4298-a7ef-e7a525019ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FYP/poon0064/.conda/envs/RunJupyter/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "epoch 0  | loss: 0.29513 | val_accuracy: 0.56579 |  0:21:06s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Define your target and feature list\n",
    "target_column = 'activation'\n",
    "remove_list = ['activation', 'ind_recommended']\n",
    "\n",
    "target_df = df[target_column]\n",
    "df = df.drop(remove_list, axis=1)\n",
    "\n",
    "features = [col for col in df.columns.to_list()]\n",
    "\n",
    "# Specify categorical and numerical features\n",
    "categorical_features = ['merchant_profile_01']\n",
    "numerical_features = [col for col in features if col not in categorical_features]\n",
    "\n",
    "# Find indices of categorical features for TabNet\n",
    "cat_idxs = [features.index(cat_feat) for cat_feat in categorical_features]\n",
    "\n",
    "# Calculate the number of unique values for each categorical feature\n",
    "cat_dims = [len(df[col].unique()) for col in categorical_features]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[features], target_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Preprocessing data...')\n",
    "# Initialize a StandardScaler and scale numerical features\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('scaler', StandardScaler())  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# create a pipeline to fill missing values and encode categorical variables\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# combining both pipelines\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', num_transformer, numerical_features),\n",
    "    ],  remainder='passthrough' \n",
    ")\n",
    "\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_val_scaled = preprocessor.transform(X_val)\n",
    "preprocessor_save_path = 'scaler_model/preprocessor1.joblib'\n",
    "joblib.dump(preprocessor, preprocessor_save_path)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize and configure the TabNet model\n",
    "#init_params = get_init_params(cat_idxs=cat_idxs, cat_dims=cat_dims)\n",
    "#clf = TabNetClassifier(**init_params)\n",
    "clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-2),\n",
    "                       scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='sparsemax')\n",
    "\n",
    "print(\"Start training...\")\n",
    "\n",
    "# Train the model\n",
    "clf.fit(\n",
    "    X_train=X_train_smote, y_train=y_train_smote,\n",
    "    eval_set=[(X_val_scaled, y_val.values)],\n",
    "    eval_name=['val'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=10,  # Adjust as needed\n",
    "    patience=10,  # For early stopping\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "model_save_path = 'ml_models/clf_model.zip'  # Choose your path and file name\n",
    "clf.save_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949ce7ee-2fdb-4504-8f7e-de22ddf790cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/FYP/poon0064/.conda/envs/RunJupyter/lib/python3.10/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b72721-40f5-4976-9b60-3c24a7d866e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 11:46:14.007015: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811e258-afae-4833-88b7-5057591ad281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "import joblib\n",
    "# Assuming df, target_column, and features are already defined as per your code snippet\n",
    "\n",
    "# Split data into features and target\n",
    "X = df[features].values\n",
    "y = df[target_column].values\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your categorical and numerical features again\n",
    "categorical_features = ['merchant_profile_01']\n",
    "numerical_features = [col for col in features if col not in categorical_features]\n",
    "\n",
    "# Create DataFrames for ease of use with ColumnTransformer\n",
    "X_train_df = pd.DataFrame(X_train, columns=numerical_features + categorical_features)\n",
    "X_test_df = pd.DataFrame(X_test, columns=numerical_features + categorical_features)\n",
    "\n",
    "# Define the preprocessor with one-hot encoding for categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing to training and test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train_df)\n",
    "X_test_transformed = preprocessor.transform(X_test_df)\n",
    "\n",
    "# Save the preprocessor for later use\n",
    "preprocessor_save_path = 'scaler_model/preprocessor_with_encoding.joblib'\n",
    "joblib.dump(preprocessor, preprocessor_save_path)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67a10ec4-2689-4468-95a7-d5cdd111a2d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 44152038090000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the model directly with NumPy arrays\u001b[39;00m\n\u001b[1;32m      8\u001b[0m max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# NumPy array\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# NumPy array\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_models/clf_model.zip\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Choose your path and file name\u001b[39;00m\n\u001b[1;32m     21\u001b[0m clf\u001b[38;5;241m.\u001b[39msave_model(model_save_path)\n",
      "File \u001b[0;32m~/.conda/envs/RunJupyter/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:240\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update__(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_unsupervised\u001b[38;5;241m.\u001b[39mget_params())\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warm_start:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# model has never been fitted before of warm_start is False\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_network_params()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_metrics(eval_metric, eval_names)\n",
      "File \u001b[0;32m~/.conda/envs/RunJupyter/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:603\u001b[0m, in \u001b[0;36mTabModel._set_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Setup the network and explain matrix.\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_group_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork \u001b[38;5;241m=\u001b[39m tab_network\u001b[38;5;241m.\u001b[39mTabNet(\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim,\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     group_attention_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_matrix\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    622\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreducing_matrix \u001b[38;5;241m=\u001b[39m create_explain_matrix(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39minput_dim,\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mcat_emb_dim,\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mcat_idxs,\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mpost_embed_dim,\n\u001b[1;32m    629\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/RunJupyter/lib/python3.10/site-packages/pytorch_tabnet/utils.py:310\u001b[0m, in \u001b[0;36mcreate_group_matrix\u001b[0;34m(list_groups, input_dim)\u001b[0m\n\u001b[1;32m    307\u001b[0m check_list_groups(list_groups, input_dim)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 310\u001b[0m     group_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group_matrix\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 44152038090000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-2),\n",
    "                       scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='sparsemax')\n",
    "\n",
    "# Fit the model directly with NumPy arrays\n",
    "max_epochs = 30\n",
    "clf.fit(\n",
    "    X_train=X_train_smote,  # NumPy array\n",
    "    y_train=y_train_smote,  # NumPy array\n",
    "    max_epochs=max_epochs,\n",
    "    patience=15,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "model_save_path = 'ml_models/clf_model.zip'  # Choose your path and file name\n",
    "clf.save_model(model_save_path)\n",
    "\n",
    "# Predict on test set with NumPy array\n",
    "preds_proba = clf.predict_proba(X_test_transformed)[:, 1]\n",
    "preds = (preds_proba > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5309da94-21f3-446e-98ff-1306123a8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('train_test/imputed_test.parquet')\n",
    "original_test_df = pd.read_csv('evaluation.csv')\n",
    "\n",
    "customer_merchant_info = original_test_df[['customer', 'merchant']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f6d1b-6429-46a9-98b4-40b0994bf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "preprocessor = joblib.load('scaler_model/preprocessor_with_encoding.joblib')\n",
    "X_new_transformed = preprocessor.transform(test_df[features])\n",
    "dtest_new = xgb.DMatrix(X_new_transformed)\n",
    "pred_probs = bst.predict(dtest_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df8938-2c66-4f7c-86dd-27a7847d72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\n",
    "    'predicted_score': pred_probs\n",
    "})\n",
    "\n",
    "# Concatenate the Customer and Merchant columns with the predictions\n",
    "# Make sure that the indices align correctly\n",
    "final_output = pd.concat([customer_merchant_info.reset_index(drop=True), predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e69cb-cde5-4ec7-9703-fbaa65f8f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def save_new_submission(df, base_path='submission/', base_filename='final_submission'):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a new CSV file, incrementing the file name index to avoid overwrites.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to save.\n",
    "    - base_path: The directory where the file should be saved.\n",
    "    - base_filename: The base name for the file, without index and extension.\n",
    "    \"\"\"\n",
    "    # Ensure the base_path exists\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "\n",
    "    # Regular expression to match files with the pattern: base_filename_x.csv\n",
    "    pattern = re.compile(rf'^{base_filename}_([0-9]+)\\.csv$')\n",
    "\n",
    "    # Get a list of all files in the base_path\n",
    "    files = os.listdir(base_path)\n",
    "\n",
    "    # Find all files matching the pattern and extract their indices\n",
    "    indices = [int(pattern.match(file).group(1)) for file in files if pattern.match(file)]\n",
    "\n",
    "    # Determine the next index (start with 1 if no files are found)\n",
    "    next_index = max(indices) + 1 if indices else 1\n",
    "\n",
    "    # Construct the new file name with the next index\n",
    "    new_file_name = f'{base_filename}_{next_index}.csv'\n",
    "\n",
    "    # Full path for the new file\n",
    "    full_path = os.path.join(base_path, new_file_name)\n",
    "\n",
    "    # Save the DataFrame to the new file\n",
    "    df.to_csv(full_path, index=False)\n",
    "    print(f'File saved as: {full_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d147f3-3d0c-469e-881d-6afc84ed4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_new_submission(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
