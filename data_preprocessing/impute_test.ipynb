{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19b1fd2-db61-4c28-90b4-41f7a1e31e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d352de-43fb-48d8-bb89-9fa8f91c950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= cudf.read_csv('evaluation.csv')\n",
    "# columns_to_remove = [\n",
    "#     'customer_merchant_02', 'customer_merchant_01',\n",
    "#     'customer_digital_activity_07', 'customer_digital_activity_08',\n",
    "#     'customer_digital_activity_09', 'customer_digital_activity_18', 'customer', 'merchant'\n",
    "# ]\n",
    "\n",
    "# # Assuming 'data' is your cuDF DataFrame\n",
    "# df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# initial_parquet_path = 'imputed_test_df_0.parquet'\n",
    "# df.to_parquet(initial_parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8201233-c56a-4158-9e4f-4eef75601410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "def batch_impute_and_save_testset(test_df_path, model_directory, batch_size, base_filename, progress_tracker_file):\n",
    "    \"\"\"\n",
    "    Perform or continue imputation on batches of columns for the test dataset using pre-trained models and save intermediate results.\n",
    "    \n",
    "    Args:\n",
    "    test_df_path (str): Path to the test dataset (Parquet file) to impute.\n",
    "    model_directory (str): Directory where the pre-trained models are saved.\n",
    "    numerical_categories (list): List of columns to impute.\n",
    "    exclude_columns (list): List of columns to exclude from imputation.\n",
    "    batch_size (int): Number of columns to impute per batch.\n",
    "    base_filename (str): Base name for saving intermediate Parquet files.\n",
    "    progress_tracker_file (str): File to track the progress of imputation.\n",
    "    \"\"\"\n",
    "    # Determine start index based on progress tracker file\n",
    "    start_index = 0\n",
    "    if os.path.exists(progress_tracker_file):\n",
    "        with open(progress_tracker_file, 'r') as f:\n",
    "            start_index = int(f.read().strip())\n",
    "        test_df_path = f\"test/{base_filename}_{start_index}.parquet\"\n",
    "    \n",
    "    # Load the DataFrame\n",
    "    df = cudf.read_parquet(test_df_path)\n",
    "\n",
    "    exclude_columns = ['distance_05']\n",
    "    categories = df.columns.to_list()\n",
    "    for i in exclude_columns:\n",
    "        categories.remove(i)\n",
    "    \n",
    "    for i, col in enumerate(categories[start_index:], start=start_index):\n",
    "        if col in exclude_columns:\n",
    "            continue  # Skip excluded columns\n",
    "        \n",
    "        model_path = f\"{model_directory}/{col}_model.pkl\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"No model found for column {col}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load the model just-in-time\n",
    "        model = joblib.load(model_path)\n",
    "        print(f\"Start imputing for column: {col}\")\n",
    "        # Impute missing values for the column\n",
    "        missing_mask = df[col].isnull()\n",
    "        if missing_mask.sum() > 0:\n",
    "            X_missing = df.loc[missing_mask].drop(columns=[col]).fillna(0)\n",
    "            df.loc[missing_mask, col] = model.predict(X_missing)\n",
    "        \n",
    "        print(f'Successfully imputed values for column: {col}')\n",
    "        \n",
    "        # Save after every batch or at the end\n",
    "        if (i + 1) % batch_size == 0 or i == len(categories) - 1:\n",
    "            save_path = f'test/{base_filename}_{i + 1}.parquet'\n",
    "            df.to_parquet(save_path)\n",
    "            with open(progress_tracker_file, 'w') as f:\n",
    "                f.write(str(i + 1))\n",
    "            print(f'Saved intermediate results to {save_path}')\n",
    "            if i < len(categories) - 1:\n",
    "                return  # Allows the process to be paused and resumed\n",
    "\n",
    "    print(\"Imputation completed.\")\n",
    "\n",
    "# Parameters for the function call\n",
    "test_df_path = 'test/imputed_test_df_0.parquet'\n",
    "model_directory = 'impute_models'\n",
    "batch_size = 15\n",
    "base_filename = 'imputed_test_df'\n",
    "progress_tracker_file = 'test_imputation_progress.txt'\n",
    "\n",
    "# Start or continue the imputation\n",
    "batch_impute_and_save_testset(test_df_path, model_directory, batch_size, base_filename, progress_tracker_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78fa1d-8098-42be-918b-8bc4ff65316e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
